{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM Spam Detection\n",
    "Below a LSTM model is trained on the SMS Spam dataset. GloVe is used for word embeddings.\n",
    "\n",
    "data: https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection\n",
    "\n",
    "word embeddings: https://nlp.stanford.edu/projects/glove/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configs\n",
    "\n",
    "EMBEDDING_DIM = 50\n",
    "SENTENCE_LENGTH = 256\n",
    "GLOVE_FILE = f'glove.6b/glove.6B.{EMBEDDING_DIM}d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load SMS Spam Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>data</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ham</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ham</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                               data\n",
       "0   ham  Go until jurong point, crazy.. Available only ...\n",
       "1   ham                      Ok lar... Joking wif u oni...\n",
       "2  spam  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "3   ham  U dun say so early hor... U c already then say...\n",
       "4   ham  Nah I don't think he goes to usf, he lives aro..."
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('./data/SMSSpamCollection', sep='\\t', names=['label', 'data'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load GloVe Word Embeddings\n",
    "https://nlp.stanford.edu/projects/glove/\n",
    "\"GloVe is an unsupervised learning algorithm for obtaining vector representations for words. Training is performed on aggregated global word-word co-occurrence statistics from a corpus, and the resulting representations showcase interesting linear substructures of the word vector space.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "embeddings = {}\n",
    "\n",
    "with open(GLOVE_FILE, 'r', encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        values = line.strip().split()\n",
    "        w = values[0]\n",
    "        vectors = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings[w] = vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OOV Tokens\n",
    "We will use the average of our word embeddings for out of vocabulary tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.12920076 -0.28866628 -0.01224866 -0.05676644 -0.20210965 -0.08389011\n",
      "  0.33359843  0.16045167  0.03867431  0.17833012  0.04696583 -0.00285802\n",
      "  0.29099807  0.04613704 -0.20923874 -0.06613114 -0.06822549  0.07665912\n",
      "  0.3134014   0.17848536 -0.1225775  -0.09916984 -0.07495987  0.06413227\n",
      "  0.14441176  0.60894334  0.17463093  0.05335403 -0.01273871  0.03474107\n",
      " -0.8123879  -0.04688699  0.20193407  0.2031118  -0.03935686  0.06967544\n",
      " -0.01553638 -0.03405238 -0.06528071  0.12250231  0.13991883 -0.17446303\n",
      " -0.08011883  0.0849521  -0.01041659 -0.13705009  0.20127155  0.10069408\n",
      "  0.00653003  0.01685157]\n"
     ]
    }
   ],
   "source": [
    "# Get number of vectors and hidden dim\n",
    "with open(GLOVE_FILE, 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        pass\n",
    "n_vec = i + 1\n",
    "hidden_dim = len(line.split(' ')) - 1\n",
    "\n",
    "vecs = np.zeros((n_vec, hidden_dim), dtype=np.float32)\n",
    "\n",
    "with open(GLOVE_FILE, 'r') as f:\n",
    "    for i, line in enumerate(f):\n",
    "        vecs[i] = np.array([float(n) for n in line.split(' ')[1:]], dtype=np.float32)\n",
    "\n",
    "average_vec = np.mean(vecs, axis=0)\n",
    "print(average_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 4.1800e-01,  2.4968e-01, -4.1242e-01,  1.2170e-01,  3.4527e-01,\n",
       "       -4.4457e-02, -4.9688e-01, -1.7862e-01, -6.6023e-04, -6.5660e-01,\n",
       "        2.7843e-01, -1.4767e-01, -5.5677e-01,  1.4658e-01, -9.5095e-03,\n",
       "        1.1658e-02,  1.0204e-01, -1.2792e-01, -8.4430e-01, -1.2181e-01,\n",
       "       -1.6801e-02, -3.3279e-01, -1.5520e-01, -2.3131e-01, -1.9181e-01,\n",
       "       -1.8823e+00, -7.6746e-01,  9.9051e-02, -4.2125e-01, -1.9526e-01,\n",
       "        4.0071e+00, -1.8594e-01, -5.2287e-01, -3.1681e-01,  5.9213e-04,\n",
       "        7.4449e-03,  1.7778e-01, -1.5897e-01,  1.2041e-02, -5.4223e-02,\n",
       "       -2.9871e-01, -1.5749e-01, -3.4758e-01, -4.5637e-02, -4.4251e-01,\n",
       "        1.8785e-01,  2.7849e-03, -1.8411e-01, -1.1514e-01, -7.8581e-01],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample embedding\n",
    "embeddings['the']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "ps = PorterStemmer()\n",
    "translator = str.maketrans('', '', string.punctuation)\n",
    "\n",
    "def normalize_word(text):\n",
    "    # Remove white space, cast to lowercase, \n",
    "    # remove punctuation and numbers\n",
    "    text = text.lower()\n",
    "    text = text.translate(translator)\n",
    "    text = text.strip(' ')\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "#     text = ps.stem(text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encode Text into Numpy Array\n",
    "This process is done by encoding each word of each sentence to its word vector. We pad our sentences with a 0 matrix to make them all the same length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "WARN = True\n",
    "def encode_sentence(sentence, embeddings, sentence_length=SENTENCE_LENGTH):\n",
    "    encoded_sentence = []\n",
    "    words = list(map(lambda w: normalize_word(w), sentence.split(' ')))\n",
    "    for word in words:\n",
    "        if word == '':\n",
    "            continue\n",
    "        if len(encoded_sentence) >= sentence_length:\n",
    "            break\n",
    "        if word in embeddings:\n",
    "            word_embedding = embeddings[word]\n",
    "        else:\n",
    "            word_embedding = average_vec\n",
    "#             continue\n",
    "    \n",
    "        encoded_sentence.append(word_embedding)\n",
    "        \n",
    "    # Zero Pad embeddings to sentence_length for LSTM batch training\n",
    "    while len(encoded_sentence) < sentence_length:\n",
    "        encoded_sentence.append(np.zeros((EMBEDDING_DIM)))\n",
    "    return np.array(encoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.38497001,  0.80092001,  0.064106  , -0.28354999, -0.026759  ,\n",
       "        -0.34531999, -0.64253002, -0.11729   , -0.33256999,  0.55242997,\n",
       "        -0.087813  ,  0.90350002,  0.47102001,  0.56656998,  0.69849998,\n",
       "        -0.35229   , -0.86541998,  0.90573001,  0.03576   , -0.071705  ,\n",
       "        -0.12327   ,  0.54922998,  0.47005001,  0.35572001,  1.26110005,\n",
       "        -0.67580998, -0.94983   ,  0.68665999,  0.38710001, -1.34920001,\n",
       "         0.63511997,  0.46416   , -0.48813999,  0.83827001, -0.92460001,\n",
       "        -0.33722001,  0.53741002, -1.06159997, -0.081403  , -0.67110997,\n",
       "         0.30923   , -0.39230001, -0.55001998, -0.68826997,  0.58048999,\n",
       "        -0.11626   ,  0.013139  , -0.57653999,  0.048833  ,  0.67203999],\n",
       "       [ 0.68491   ,  0.32385001, -0.11592   , -0.35925001,  0.49889001,\n",
       "         0.042541  , -0.40153   , -0.36793   , -0.61440998, -0.41148001,\n",
       "        -0.34819999, -0.21952   , -0.22393   , -0.64965999,  0.85443002,\n",
       "         0.33581999,  0.2931    ,  0.16552   , -0.55081999, -0.61277002,\n",
       "        -0.14768   ,  0.47551   ,  0.65877002, -0.07103   ,  0.56146997,\n",
       "        -1.2651    , -0.74116999,  0.36364999,  0.56230003, -0.27364999,\n",
       "         3.8506    ,  0.27645001, -0.1009    , -0.71568   ,  0.18511   ,\n",
       "        -0.12312   ,  0.56630999, -0.22376999, -0.016831  ,  0.57538998,\n",
       "        -0.51761001,  0.033823  ,  0.19643   ,  0.63498002, -0.24866   ,\n",
       "         0.038716  , -0.50559002,  0.17873999, -0.1693    ,  0.062375  ],\n",
       "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
       "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sample embedded sentence zero padded to a length of 3\n",
    "encode_sentence(\"hello there\", embeddings, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_X(dfx, embeddings):\n",
    "    encoded_df = []\n",
    "    for x in dfx.values:\n",
    "        sentence_embedding = encode_sentence(x, embeddings)\n",
    "        encoded_df.append(sentence_embedding)\n",
    "    np.concatenate(encoded_df, axis=0)\n",
    "    return np.array(encoded_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split our data into Training, Validation, and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     4825\n",
       "spam     747\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Shuffle df\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['data']\n",
    "X_train = X[0:int(X.shape[0]*.6)]\n",
    "X_val = X[int(X.shape[0]*.6):int(X.shape[0]*.75)]\n",
    "X_test = X[int(X.shape[0]*.75):]\n",
    "\n",
    "X_train = encode_X(X_train, embeddings)\n",
    "X_val = encode_X(X_val, embeddings)\n",
    "X_test = encode_X(X_test, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ham     2891\n",
      "spam     452\n",
      "Name: label, dtype: int64\n",
      "ham     722\n",
      "spam    114\n",
      "Name: label, dtype: int64\n",
      "ham     1212\n",
      "spam     181\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "y = df['label']\n",
    "y_train = y[0:int(y.shape[0]*.6)]\n",
    "y_val = y[int(y.shape[0]*.6):int(y.shape[0]*.75)]\n",
    "y_test = y[int(y.shape[0]*.75):]\n",
    "\n",
    "print(y_train.value_counts())\n",
    "print(y_val.value_counts())\n",
    "print(y_test.value_counts())\n",
    "\n",
    "HAM = 0\n",
    "SPAM = 1\n",
    "\n",
    "y_train = np.array(list(map(lambda x: HAM if x == 'ham' else SPAM, y_train)))\n",
    "y_val = np.array(list(map(lambda x: HAM if x == 'ham' else SPAM, y_val)))\n",
    "y_test = np.array(list(map(lambda x: HAM if x == 'ham' else SPAM, y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1393, 256, 50)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(X_test.shape)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.LSTM(256, input_shape=(SENTENCE_LENGTH, EMBEDDING_DIM)),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "])\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=tf.keras.optimizers.Adam(2e-5),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3343 samples, validate on 836 samples\n",
      "Epoch 1/10\n",
      "3343/3343 [==============================] - 68s 20ms/sample - loss: 0.6668 - accuracy: 0.8648 - val_loss: 0.5388 - val_accuracy: 0.8659\n",
      "Epoch 2/10\n",
      "3343/3343 [==============================] - 61s 18ms/sample - loss: 0.3337 - accuracy: 0.8720 - val_loss: 0.2714 - val_accuracy: 0.9281\n",
      "Epoch 3/10\n",
      "3343/3343 [==============================] - 63s 19ms/sample - loss: 0.1871 - accuracy: 0.9459 - val_loss: 0.1961 - val_accuracy: 0.9496\n",
      "Epoch 4/10\n",
      "3343/3343 [==============================] - 65s 19ms/sample - loss: 0.1700 - accuracy: 0.9459 - val_loss: 0.2005 - val_accuracy: 0.9485\n",
      "Epoch 5/10\n",
      "3343/3343 [==============================] - 63s 19ms/sample - loss: 0.1625 - accuracy: 0.9527 - val_loss: 0.1912 - val_accuracy: 0.9506\n",
      "Epoch 6/10\n",
      "3343/3343 [==============================] - 67s 20ms/sample - loss: 0.1511 - accuracy: 0.9545 - val_loss: 0.1738 - val_accuracy: 0.9506\n",
      "Epoch 7/10\n",
      "3343/3343 [==============================] - 54s 16ms/sample - loss: 0.1453 - accuracy: 0.9545 - val_loss: 0.1542 - val_accuracy: 0.9539\n",
      "Epoch 8/10\n",
      "3343/3343 [==============================] - 36s 11ms/sample - loss: 0.1408 - accuracy: 0.9512 - val_loss: 0.1635 - val_accuracy: 0.9528\n",
      "Epoch 9/10\n",
      "3343/3343 [==============================] - 32s 10ms/sample - loss: 0.1311 - accuracy: 0.9632 - val_loss: 0.1523 - val_accuracy: 0.9528\n",
      "Epoch 10/10\n",
      "2752/3343 [=======================>......] - ETA: 5s - loss: 0.1301 - accuracy: 0.9626"
     ]
    }
   ],
   "source": [
    "history = model.fit(x=X_train, y=y_train, epochs=10,\n",
    "                    validation_data=(X_val, y_val),\n",
    "                    shuffle=True,\n",
    "                    validation_steps=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "training_loss = history.history['loss']\n",
    "test_loss = history.history['val_loss']\n",
    "epoch_count = range(1, len(training_loss) + 1)\n",
    "\n",
    "plt.plot(epoch_count, training_loss, 'r--')\n",
    "plt.plot(epoch_count, test_loss, 'b-')\n",
    "plt.legend(['Training Loss', 'Test Loss'])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = model.test_on_batch(\n",
    "    X_test,\n",
    "    y=y_test,\n",
    "    sample_weight=None,\n",
    "    reset_metrics=True\n",
    ")\n",
    "print(list(zip(model.metrics_names, test_results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test).round()\n",
    "y_pred = list(map(lambda x: int(x[0]), y_pred))\n",
    "confusion = tf.math.confusion_matrix(labels=y_test, predictions=y_pred, num_classes=2)\n",
    "confusion = confusion.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fp = confusion[0][1]\n",
    "tn = confusion[0][0]\n",
    "fp_rate = float(fp / (fp+tn))\n",
    "print(f\"False Positive Rate: {round(fp_rate*100,2)}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "df_cm = pd.DataFrame(confusion, index = [\"HAM\", \"SPAM\"],\n",
    "                  columns = [\"HAM\", \"SPAM\"])\n",
    "plt.figure(figsize = (12,8))\n",
    "sns.set(font_scale=1.5)\n",
    "annot_kws = {\"ha\": 'left',\"va\": 'bottom'}\n",
    "hm = sns.heatmap(df_cm, cmap=\"Pastel1\", fmt=\"d\", annot=True, annot_kws=annot_kws)\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Sample Sentences\n",
    "This function allows us to make predictions on arbitrary sentences using the trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_sentence(sentence):\n",
    "    s = encode_sentence(sentence, embeddings)\n",
    "    s = s.reshape((1,s.shape[0],s.shape[1]))\n",
    "    prediction = model.predict(s)[0][0]\n",
    "    if prediction >= 0.5:\n",
    "        print(\"It's SPAM!\")\n",
    "    else:\n",
    "        print(\"It's HAM!\")\n",
    "    print(f\"Sigmoid: {prediction}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentence(\"\"\"Enter to win $25,000 and get a Free Hotel Night! Just click \n",
    "    here for a $1 trial membership in NetMarket, the Internet's\n",
    "    premier discount shopping site:\n",
    "    http://www.netmarket.com/MEM/scripts/outsidePromo.asp?ref=yoyo_shp01\"\"\")\n",
    "test_sentence(\"\"\"My system has been down for the past 48 hours. Please help.\"\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
